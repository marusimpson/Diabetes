# Diabetes
# -*- coding: utf-8 -*-
"""Copia de EstudioDiabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RJRZpEffArdQ6ifWIh_NylVTqXWipkDF

Se utilizó la Pima Indians Diabetes Database (PIDD),
cuya propiedad original pertenece al National Institute of
Diabetes and Digestive and Kidney Diseases, y los datos fueron
obtenidos del UCI Machine Learning Repository - Pima
Indians Diabetes Data Set (2016). Las unidades de análisis
consistieron en 768 mujeres residentes cerca de Phoenix,
Arizona, EEUU, pertenecientes a la etnia Pima y con al menos
21 años de edad.
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Leer el archivo Excel (con extensión .xlsx)
df = pd.read_csv("diabetes.csv")

# Mostrar las primeras filas
print("Primeras filas del dataset:")
print(df.head())

"""Ver cantidad de filas y columnas"""

df.shape

"""Descripción de los datos:

1. Embarazos: Cantidad de embarazos  de la paciente

2. Glucosa: Concentración de glucosa plasmática a las 2hs de una
prueba de tolerancia oral a la glucosa (G120 mg/dl)

3. Presion Arterial: Presión arterial diastólica (PAD mmHg.)

4. Espesor de Piel: Grosor del pliegue de la piel del tríceps (GPPT mm)

5. Insulina: Concentración de insulina sérica a las 2hs de una prueba
de tolerancia oral a la glucosa (I120 mU/ml)

6. IMC

7. Función de Ascendencia Diabética ó Función pedigrí de la diabetes: Antecedentes Familiares o función de pedigrí de diabetes
(FPD)

8. Edad: Tiempo de vida de la persona.

9. Diabetes ó Resultado: Esta variable determina si la persona tiene Diabetes o no la tiene.
"""

df

"""Vemos las claves del dataframe. En mi caso edite el archico CSV antes de cargarlo"""

df.keys()

df.columns

"""Comprobamos que no existan datos faltantes"""

df.info()

"""Resumen estadistico"""

df.describe()

"""Estos son algunos puntos que podemos notar sobre los datos:

* Todas nuestras características son numéricas
* Tenemos un tamaño de muestra total de 768
* No faltan valores con los que lidiar en este momento
* Algunas características tienen un valor mínimo de 0 que es sospechoso para los humanos (vivos):

  - Mínima glucosa = 0          (5   casos)
  - Presión arterial mínima = 0 (35  casos)
  - Espesor de piel mínimo = 0  (227 casos)
  - Min insulina = 0            (374 casos)
  - Min ICM = 0                 (11  casos)

  En grupo se decide si convertimos estos ceros en nulos y los eliminamos (quedarian 392 datos) o ponemos una media
"""

columnas_revisar = ['Glucosa', 'PresionArterial', 'GrosorPiel', 'Insulina', 'IMC']

# Crear un DataFrame con el conteo de ceros por columna
ceros_por_columna = (df[columnas_revisar] == 0).sum()

# Mostrar el resultado
print("Cantidad de ceros por columna:")
print(ceros_por_columna)

"""Hago una cuadricula de histogramas para observasr las variables numericas"""

df.hist(figsize=(12,8))
plt.show()

"""Comprobamos la cantidad de datos faltantes de mi dataset"""

df.isnull().sum()

"""MATRIZ DE CORRELACIONES

Observamos la correlación de las variables con la variable DIABETES (ultimna columna).

Es para analizar en grupo
"""

df.corr().Diabetes.sort_values()

import seaborn as sns
correlation_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Matriz de Correlación')
plt.show()

from pandas.plotting import scatter_matrix
scatter_matrix(df, figsize=(14,10))

plt.savefig('scatter_diabetes.jpg')
plt.show()

"""A CREAR DATA TRAIN Y TEST!!!

TRAIN: 80% -TEST: 20%

Creamos las variables y separamos

X (predictoras) (¿Por convención x mayuscula?)

y (etiquetas)
"""

X = df.drop('Diabetes', axis=1) #Quitamos la variable respuesta Diabetes
y = df.Diabetes # Variable respuesta

from sklearn.model_selection import train_test_split

"""Dividisión en entrenamiento y testeo"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Reemplazar los ceros solo después de dividir, usando la media del conjunto de entrenamiento"""

from sklearn.impute import SimpleImputer

# Columnas donde los ceros deben ser reemplazados
columnas_a_corregir = ['Glucosa', 'PresionArterial', 'GrosorPiel', 'Insulina', 'IMC']
# Crear el imputador: reemplazará ceros por la media
imputador = SimpleImputer(missing_values=0, strategy='mean')
# Ajustar el imputador solo con los datos de entrenamiento
X_train[columnas_a_corregir] = imputador.fit_transform(X_train[columnas_a_corregir])
# Aplicar el mismo imputador al conjunto de prueba
X_test[columnas_a_corregir] = imputador.transform(X_test[columnas_a_corregir])

X_train.describe()

"""ENTRENAMIENTO DE VARIOS MODELOS DE CLASIFICACIÓN"""

from sklearn.linear_model import SGDClassifier # Clasificador lineal Descenso de Gradiente Estocástico (Stochastic Gradient Descent)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC # Máquina de vectores de soporte
from sklearn.neighbors import KNeighborsClassifier # Vecinos mas cercanos
from sklearn.linear_model import LogisticRegression # Regresión Logistica

"""Instanciación de las seis clases"""

sgd_clf = SGDClassifier()
tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)
forest_clf = RandomForestClassifier()
svc_clf = SVC()
knn_clf = KNeighborsClassifier(n_neighbors= 5) # 5 vecinos queremos que utilice
lr_clf = LogisticRegression()

"""Entrenamiento de los modelos"""

sgd_clf.fit(X_train, y_train)
tree_clf.fit(X_train, y_train)
forest_clf.fit(X_train, y_train)
svc_clf.fit(X_train, y_train)
knn_clf.fit(X_train, y_train)
lr_clf.fit(X_train, y_train)

"""Evaluación de desempeño de los modelos con la matriz de confusión"""

#from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score, confusion_matrix

y_train_prediccion_sgd = cross_val_predict(sgd_clf, X_train, y_train, cv = 5)
y_train_prediccion_tree = cross_val_predict(tree_clf, X_train, y_train, cv = 5)
y_train_prediccion_forest = cross_val_predict(forest_clf, X_train, y_train, cv = 5)
y_train_prediccion_svc = cross_val_predict(svc_clf, X_train, y_train, cv = 5)
y_train_prediccion_knn = cross_val_predict(knn_clf, X_train, y_train, cv = 5)
y_train_prediccion_lr = cross_val_predict(lr_clf, X_train, y_train, cv = 5)

"""Descenso de Gradiente Estocástico"""

print(confusion_matrix(y_train, y_train_prediccion_sgd))
print(f'Sensibilidad: {recall_score(y_train, y_train_prediccion_sgd)}')
print(f'Accuracy: {accuracy_score(y_train, y_train_prediccion_sgd)}')
print(f'Precision: {precision_score(y_train, y_train_prediccion_sgd)}')
print(f'F1 Score: {f1_score(y_train, y_train_prediccion_sgd)}')

"""ÁRBOL DE DECISIÓN"""

print(confusion_matrix(y_train, y_train_prediccion_tree))
print(f'Sensibilidad: {recall_score(y_train, y_train_prediccion_tree)}')
print(f'Accuracy: {accuracy_score(y_train, y_train_prediccion_tree)}')
print(f'Precision: {precision_score(y_train, y_train_prediccion_tree)}')
print(f'F1 Score: {f1_score(y_train, y_train_prediccion_tree)}')

"""RANDOM FOREST"""

print(confusion_matrix(y_train, y_train_prediccion_forest))
print(f'Sensibilidad: {recall_score(y_train, y_train_prediccion_forest)}')
print(f'Accuracy: {accuracy_score(y_train, y_train_prediccion_forest)}')
print(f'Precision: {precision_score(y_train, y_train_prediccion_forest)}')
print(f'F1 Score: {f1_score(y_train, y_train_prediccion_forest)}')

"""Máquina de vectores de soporte"""

print(confusion_matrix(y_train, y_train_prediccion_svc))
print(f'Sensibilidad: {recall_score(y_train, y_train_prediccion_svc)}')
print(f'Accuracy: {accuracy_score(y_train, y_train_prediccion_svc)}')
print(f'Precision: {precision_score(y_train, y_train_prediccion_svc)}')
print(f'F1 Score: {f1_score(y_train, y_train_prediccion_svc)}')

"""KNN (Vecinos mas cercanos)"""

print(confusion_matrix(y_train, y_train_prediccion_knn))
print(f'Sensibilidad: {recall_score(y_train, y_train_prediccion_knn)}')
print(f'Accuracy: {accuracy_score(y_train, y_train_prediccion_knn)}')
print(f'Precision: {precision_score(y_train, y_train_prediccion_knn)}')
print(f'F1 Score: {f1_score(y_train, y_train_prediccion_knn)}')

"""REGRESIÓN LOGISTICA"""

print(confusion_matrix(y_train, y_train_prediccion_lr))
print(f'Sensibilidad: {recall_score(y_train, y_train_prediccion_lr)}')
print(f'Accuracy: {accuracy_score(y_train, y_train_prediccion_lr)}')
print(f'Precision: {precision_score(y_train, y_train_prediccion_lr)}')
print(f'F1 Score: {f1_score(y_train, y_train_prediccion_lr)}')

"""Graficamos el arbol de decision:
*   GINI: medida de impureza (falta de homogeneidad). Cuanto mayor sea el valor del Índice de Gini, mayor será la probabilidad de que se produzcan clasificaciones erróneas.
*   SAMPLES: número de observaciones o ejemplos (filas de datos) que llegan a un nodo específico del árbol durante el entrenamiento.
*   VALUE: es una lista (o vector) que muestra el número de muestras de cada clase presentes en ese nodo.
"""

from sklearn.tree import plot_tree
# Paso 7: Visualizar el árbol
plt.figure(figsize=(20,10))
plot_tree(tree_clf, feature_names=X.columns, class_names=["No Diabetes", "Diabetes"], filled=True, proportion=True)
plt.title("Árbol de decisión para detección de diabetes")
plt.show()

"""Graficamos RANDOM FOREST"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Seleccionar uno de los árboles (por ejemplo, el primero)
tree = forest_clf.estimators_[0]

# Graficar el árbol
plt.figure(figsize=(20,10))
plot_tree(tree, feature_names=X.columns, class_names=["No Diabetes", "Diabetes"], filled=True, rounded=True)
plt.title("Un árbol dentro del Random Forest")
plt.show()

import joblib

# Guardar el modelo en un archivo
joblib.dump(tree_clf, 'modelo_arbol_decision.pkl')

# Descargar el archivo a tu computadora
from google.colab import files
files.download('modelo_arbol_decision.pkl')

"""ARMAMOS LA WEB"""

!pip install streamlit pyngrok scikit-learn joblib --quiet

from google.colab import files
uploaded = files.upload()

import streamlit as st
import joblib
import numpy as np
from datetime import datetime

# Cargar modelo
modelo = joblib.load('modelo_arbol_decision.pkl')

# ================== ESTILOS BÁSICOS ==================
st.markdown("""
    <style>
        body, .main {
            background-color: #ffffff;
            color: #000000;
            font-family: sans-serif;
        }
        .top-bar {
            background: #005b96;
            color: white;
            padding: 1rem 2rem;
            font-weight: bold;
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }
        .resultado {
            padding: 1.5rem;
            border-radius: 10px;
            font-size: 1.2rem;
            margin-top: 2rem;
            text-align: center;
        }
        .alto {
            background-color: #ffe5e5;
            border: 2px solid #c5221f;
            color: #c5221f;
        }
        .bajo {
            background-color: #e0fff2;
            border: 2px solid #008a5e;
            color: #008a5e;
        }
    </style>
""", unsafe_allow_html=True)

# ================== ENCABEZADO ==================
st.markdown('<div class="top-bar">DiabetesScan Pro</div>', unsafe_allow_html=True)

# ================== FORMULARIO ==================
with st.form("formulario"):
    st.subheader("📋 Ingresar datos del paciente")

    col1, col2 = st.columns(2)

    with col1:
        embarazos = st.number_input("N° de embarazos", min_value=0, step=1)
        edad = st.number_input("Edad", min_value=21, step=1)
        pedigree = st.number_input("Historial Familiar (función de pedigrí)", min_value=0.0, step=0.01)

    with col2:
        glucosa = st.number_input("Glucosa (mg/dL)", min_value=0.0, step=1.0)
        presion = st.number_input("Presión Arterial (mmHg)", min_value=0.0, step=1.0)
        piel = st.number_input("Pliegue cutáneo (mm)", min_value=0.0, step=1.0)
        insulina = st.number_input("Insulina (μU/mL)", min_value=0.0, step=1.0)
        imc = st.number_input("IMC (kg/m²)", min_value=0.0, step=0.1)

    analizar = st.form_submit_button("Analizar Riesgo")

# ================== PREDICCIÓN ==================
if analizar:
    entrada = np.array([[embarazos, glucosa, presion, piel, insulina, imc, pedigree, edad]])
    prediccion = modelo.predict(entrada)[0]

    if prediccion == 1:
        st.markdown('<div class="resultado alto"><strong>🔴 Alto riesgo de diabetes</strong></div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="resultado bajo"><strong>🟢 Bajo riesgo de diabetes</strong></div>', unsafe_allow_html=True)

!nohup streamlit run app.py &>/dev/null &

from pyngrok import ngrok

# Si ya pusiste tu token anteriormente, omite esta línea
ngrok.set_auth_token("2xygsROLh7qNKOm07pUfAvU8Tyy_6MrKmf2HwUU2KpsBBojGG")

# Matar túneles anteriores y crear uno nuevo
ngrok.kill()
public_url = ngrok.connect(8501)
print("✅ Tu app está disponible en:", public_url)
